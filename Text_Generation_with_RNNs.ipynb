{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation with LSTM-based Recurrent Neural Networks\n",
        "\n",
        "### Project Description\n",
        "This project demonstrates text generation using Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units.\n",
        "\n",
        "The goal is to train an RNN model to learn the structure of a given input text and generate new text that resembles the original text. This technique is commonly used in natural language processing (NLP) tasks such as language modeling, text summarization, and dialogue generation.\n",
        "\n",
        "### Code Overview\n",
        "The code consists of several main sections:\n",
        "1. **Data Preprocessing**: The input text is loaded from a file (`text.txt`) and preprocessed to remove unnecessary characters, tokenize the text, and build a vocabulary.\n",
        "2. **Dataset Preparation**: The preprocessed text is converted into sequences of indices representing words and organized into data samples for training.\n",
        "3. **Model Definition**: The WordLSTM class defines the architecture of the LSTM-based RNN model, including embedding layer, LSTM layer, and fully connected layer.\n",
        "4. **Model Training**: The model is trained using the prepared dataset, and the training process is executed for a specified number of epochs.\n",
        "5. **Generating Text**: After training, the model can be used to generate new text by providing a seed sequence as input and predicting the next words iteratively.\n",
        "\n"
      ],
      "metadata": {
        "id": "hQM8ceP2tjDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3els0WsPukrj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "MlJZBst_uGj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6pKb62JtiFl",
        "outputId": "cf7b3558-323a-4915-dfbd-d599315efc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chapter', '1', 'Happy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way.', 'Everything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\", 'house.', 'The', 'wife', 'had', 'discovered', 'that', 'the']\n"
          ]
        }
      ],
      "source": [
        "# Loading and preprocessing the text data\n",
        "with open('text.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text.split()[:30])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the text and cleaning it\n",
        "punctuation = [',', '.', ':', ';', '?', '!', '(', ')', '[', ']', '{', '}', '\"', '\\'', '\\\\', '/']\n",
        "def clean_text(text):\n",
        "  text = text.lower().replace('\\n', ' ')\n",
        "  text = text.replace('-', ' ')\n",
        "  for punc in punctuation:\n",
        "    text = text.replace(f'{punc}', f' {punc} ')\n",
        "  return text.split()"
      ],
      "metadata": {
        "id": "auD7cdTVuYmK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_text(text)\n",
        "word_counts = Counter(cleaned_text)\n",
        "word_counts.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c74TVchXvu-A",
        "outputId": "1ea4dd27-98f5-4c92-f306-2bd46b2d79a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 30994),\n",
              " ('.', 19671),\n",
              " ('the', 17554),\n",
              " ('\"', 13990),\n",
              " ('and', 12906),\n",
              " ('to', 10154),\n",
              " ('of', 8618),\n",
              " ('he', 7824),\n",
              " (\"'\", 6713),\n",
              " ('a', 6186)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting unique words and building vocabulary\n",
        "words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TO6YBWSv2QL",
        "outputId": "38b5f6d7-5116-45dd-e6dd-010d23902ef6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_text = len(cleaned_text)\n",
        "count_words = len(words)\n",
        "print('The text contains', len_text, 'words')\n",
        "print('The text contains', count_words, 'unique words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I61VNx1wDIe",
        "outputId": "cc6b8a2d-c53b-4c5a-9721-4b8ad20bbd0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text contains 436508 words\n",
            "The text contains 12971 unique words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word: index for index, word in enumerate(words)}\n",
        "index_to_word = {index: word for index, word in enumerate(words)}\n",
        "print(list(word_to_index.items())[:10])\n",
        "print(list(index_to_word.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18E6TtK3wmci",
        "outputId": "bf4f516b-f72d-4aa3-daf2-b6ab03de9031"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 0), ('.', 1), ('the', 2), ('\"', 3), ('and', 4), ('to', 5), ('of', 6), ('he', 7), (\"'\", 8), ('a', 9)]\n",
            "[(0, ','), (1, '.'), (2, 'the'), (3, '\"'), (4, 'and'), (5, 'to'), (6, 'of'), (7, 'he'), (8, \"'\"), (9, 'a')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_indices = [word_to_index[word] for word in cleaned_text]\n",
        "print(cleaned_text[:10])\n",
        "print(text_as_indices[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL3vXYZqw9lh",
        "outputId": "26ed74d0-a259-4c6b-b263-5103ec916e85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy']\n",
            "[207, 2751, 278, 2974, 82, 31, 2413, 35, 201, 685]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "uT_cSkiguONj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating training data\n",
        "len_sequence = 100\n",
        "data = []\n",
        "for i in range(0, len_text - len_sequence - 1):\n",
        "  sequence = text_as_indices[i: i + len_sequence]\n",
        "  label = text_as_indices[i + 1: i + len_sequence + 1]\n",
        "  data.append((torch.tensor(sequence), torch.tensor(label)))"
      ],
      "metadata": {
        "id": "LCtDI4duxMz1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "cc8aLBg70to8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence, label = next(iter(train_loader))\n",
        "print(sequence)\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ydU3oZ1Lf7",
        "outputId": "50303035-9d33-4dce-aa58-eedf7486e479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   6,  138, 2254,  ...,    8,   39,   86],\n",
            "        [   5,   81,   87,  ...,    7, 1312,   41],\n",
            "        [  13, 8035,    1,  ...,   65, 7112,   50],\n",
            "        ...,\n",
            "        [1243,   16,  174,  ...,    6,  316,  148],\n",
            "        [   0,    7,  135,  ..., 2761,    1,   94],\n",
            "        [  53,   43,  180,  ...,   13,  192,   35]])\n",
            "tensor([[ 138, 2254,    0,  ...,   39,   86,   31],\n",
            "        [  81,   87,    0,  ..., 1312,   41,   25],\n",
            "        [8035,    1,   10,  ..., 7112,   50,  367],\n",
            "        ...,\n",
            "        [  16,  174,    5,  ...,  316,  148,    1],\n",
            "        [   7,  135,    9,  ...,    1,   94,   95],\n",
            "        [  43,  180,  515,  ...,  192,   35,    7]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "ItFnxTesuRb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the WordLSTM model\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "class WordLSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
        "    super(WordLSTM, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.vocabulary_size = len(words)\n",
        "    self.embedding = nn.Embedding(self.vocabulary_size, self.input_size)\n",
        "    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True, dropout=dropout)\n",
        "    self.fully_connected = nn.Linear(self.hidden_size, self.vocabulary_size)\n",
        "\n",
        "  def forward(self, x, hc):\n",
        "    x = self.embedding(x)\n",
        "    x, hc = self.lstm(x, hc)\n",
        "    x = self.fully_connected(x)\n",
        "    return x, hc\n",
        "\n",
        "  def initialize_hidden_state(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    return (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
        "            weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGJ3tli8AlFb",
        "outputId": "3ef41778-d805-4f69-8afc-5277b90f76e9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the model\n",
        "model = WordLSTM(input_size=128, hidden_size=128, num_layers=3).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6vDKmrFAoSS",
        "outputId": "35fd150c-009c-4512-a5bd-c4f6c39a22c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (embedding): Embedding(12971, 128)\n",
            "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  (fully_connected): Linear(in_features=128, out_features=12971, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "x0RttNNxuTjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "Lnb2sK29A6rq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(10):\n",
        "  total_loss = 0\n",
        "  hidden_state, cell_state = model.initialize_hidden_state(batch_size)\n",
        "  for i, (sequences, labels) in enumerate(train_loader):\n",
        "    if sequences.shape[0]==batch_size:\n",
        "      sequences, labels = sequences.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs, (hidden_state, cell_state) = model(sequences, (hidden_state, cell_state))\n",
        "      loss = criterion(outputs.transpose(1,2),labels)\n",
        "      hidden_state, cell_state=hidden_state.detach(), cell_state.detach()\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "  print(f\"Epoch {epoch}: average loss = {total_loss / (i + 1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vnHAdnJBnqW",
        "outputId": "d6d88232-2baf-4541-ee44-d5769608ca94"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: average loss = 3.2860134329539203\n",
            "Epoch 1: average loss = 3.0206203146368233\n",
            "Epoch 2: average loss = 2.8694772977146483\n",
            "Epoch 3: average loss = 2.7685773876479405\n",
            "Epoch 4: average loss = 2.6954071568086153\n",
            "Epoch 5: average loss = 2.63963241919095\n",
            "Epoch 6: average loss = 2.59504464735889\n",
            "Epoch 7: average loss = 2.558801118746096\n",
            "Epoch 8: average loss = 2.5282852627486667\n",
            "Epoch 9: average loss = 2.501971535309264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "import pickle\n",
        "\n",
        "torch.save(model.state_dict(), \"wordLSTM.pth\")\n",
        "with open(\"word_to_index.p\", \"wb\") as fb:\n",
        "    pickle.dump(word_to_index, fb)"
      ],
      "metadata": {
        "id": "t18kmHtwut7-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with the trained LSTM model"
      ],
      "metadata": {
        "id": "70mk10g2MXoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "model.load_state_dict(torch.load(\"wordLSTM.pth\"))\n",
        "with open(\"/content/word_to_index.p\",\"rb\") as fb:\n",
        "    word_to_index = pickle.load(fb)\n",
        "\n",
        "index_to_word = {v:k for k,v in word_to_index.items()}"
      ],
      "metadata": {
        "id": "jgvp3IgPJaCW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, prompt, length=200):\n",
        "    model.eval()\n",
        "    text = prompt.lower().split(' ')\n",
        "    hidden_state = model.initialize_hidden_state(1)\n",
        "    length = length - len(text)\n",
        "    for i in range(0, length):\n",
        "        if len(text) <= 50:\n",
        "            x = torch.tensor([[word_to_index[w] for w in text]])\n",
        "        else:\n",
        "            x = torch.tensor([[word_to_index[w] for w in text[-50:]]])\n",
        "        inputs = x.to(device)\n",
        "        output, hidden_state = model(inputs, hidden_state)\n",
        "        logits = output[0][-1]\n",
        "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
        "        idx = np.random.choice(len(logits), p=p)\n",
        "        text.append(index_to_word[idx])\n",
        "    text = \" \".join(text)\n",
        "    for m in punctuation:\n",
        "        text = text.replace(f\" {m}\", f\"{m} \")\n",
        "    text = text.replace('\"  ', '\"')\n",
        "    text = text.replace(\"'  \", \"'\")\n",
        "    text = text.replace('\" ', '\"')\n",
        "    text = text.replace(\"' \", \"'\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "3ao3gfKKJ-Vf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, prompt='Anna and the prince'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugA00PaiLVoP",
        "outputId": "967d6fd4-9cc8-4068-c034-e617c12fbe15"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna and the prince went back to this place whom he wanted to supersede.  but her face looked she infected him in the pavilion.  she enjoyed that box began partly still more,  he built a piteous,  healthy man,  and a very lovely lady,  and dolly died foul of her,  and understood from her husband that might inevitably prevent her facts. \"you were such as a simile of society.  it would be right,  like mituh,  extricate all the children lidia ivanovna,  and his feeling and unpleasantly interested,  but so much outside her, \"pregnancy,  sickness,  mental incapacity,  indifference to his son when he had no good to him.  just as he felt continually at once liking them too,  with signs of immense composure.  on the previous day the auditing of the land,  at least perfectly sacred. \"bill,  if your wife won't give you home, \"she responded gloomily,  looking straight before her as a surprise. \"that's very hot, \"said countess nordston,  packing katavasov now to\n"
          ]
        }
      ]
    }
  ]
}